```{r  0.1}
print("All models are wrong, but some are useful.")

```

```{r  0.2}
x <- 1:2
x <- x * 10
x <- log(x)
x <- sum(x)
x <- exp(x)
x

```

```{r  0.3}
(log(0.01 ^ 200))
(200 * log(0.01))

```

```{r  0.4}
# Load the data:
# car braking distances in feet paired with speeds in km/h
# see ?cars for details
data(cars)

# fit a linear regression of distance on speed
m <- lm(dist ~ speed, data = cars)

# estimated coefficients from the model
coef(m)

# plot residuals against speed
plot(resid(m) ~ speed, data = cars)

```

```{r  0.5}
library(rmsutilityr)
dependencies <- make_package_dependency_list(
  c("coda", "mvtnorm", "devtools"), path =
    "/Library/Frameworks/R.framework/Resources/library/")

## Remove packages that have been installed
## from the vector of dependencies.
dependencies <- dependencies[!dependencies %in% 
                               as.character(installed.packages()[ , 1])]

install.packages(dependencies, type = "binary",
                 repos = "https://cran.revolutionanalytics.com",
                 lib = "/Library/Frameworks/R.framework/Resources/library/")
update.packages(lib.loc = "/Library/Frameworks/R.framework/Resources/library/",
                repos = "https://cran.revolutionanalytics.com", ask = FALSE)

library(devtools)
devtools::install_github("rmcelreath/rethinking")

```

```{r  2.1}
ways <- c(0, 3, 8, 9, 0)
ways / sum(ways)

```

```{r  2.2}
dbinom(6, size = 9, prob = 0.5)

```

```{r  2.3}
# define grid
p_grid <- seq(from = 0,
              to = 1,
              length.out = 20)

# define prior
prior <- rep(1, 20)

# compute likelihood at each value in grid
likelihood <- dbinom(6, size = 9, prob = p_grid)

# compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

```

```{r  2.4}
plot(p_grid,
     posterior,
     type = "b",
     xlab = "probability of water",
     ylab = "posterior probability")
     mtext("20 points")
     
```

```{r  2.5}
prior <- ifelse(p_grid < 0.5, 0, 1)
prior <- exp(-5 * abs(p_grid - 0.5))

```

```{r  2.6}
library(rethinking)
globe.qa <- map(
    alist(
        w ~ dbinom(9, p),  # binomial likelihood
        p ~ dunif(0, 1)     # uniform prior
    ),
    data = list(w = 6))

# display summary of quadratic approximation
precis(globe.qa)

```

```{r  2.7}
# analytical calculation
w <- 6
n <- 9
curve(dbeta(x, w + 1, n - w + 1), from = 0, to = 1)
# quadratic approximation
curve(dnorm(x, 0.67, 0.16), lty = 2, add = TRUE)

```
## Practice
### Medium
#### 2M1. 
Recall the globe tossing model from the chapter. Compute and plot
the grid approximate posterior distribution for each of the following set of
observations. In each case, assume a uniform prior for _p_.

```{r practice2M1}
##  (1) W, W, W
p_grid <- seq(from = 0,
              to = 1,
              length.out = 20)

# define prior
prior <- rep(1, 20)

# compute likelihood at each value in grid
likelihood <- dbinom(3, size = 3, prob = p_grid)

# compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)
plot(p_grid,
     posterior,
     type = "b",
     xlab = "probability of water",
     ylab = "posterior probability")
     mtext("20 points")

##     (2) W, W, W, L
# compute likelihood at each value in grid
likelihood <- dbinom(3, size = 4, prob = p_grid)

# compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)
plot(p_grid,
     posterior,
     type = "b",
     xlab = "probability of water",
     ylab = "posterior probability")
     mtext("20 points")

     ##     (3) L, W, W, L, W, W, W
# compute likelihood at each value in grid
likelihood <- dbinom(5, size = 7, prob = p_grid)

# compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)
plot(p_grid,
     posterior,
     type = "b",
     xlab = "probability of water",
     ylab = "posterior probability")
     mtext("20 points")

```

#### 2M2. 
Now assume a prior for _p_ that is equal to zero when $p < 0.5$ and
is a positive constant when $p \ge 0.5$. Again compute and plot the grid 
approximate posterior distribution for each of the sets of observations in the 
problem just above.

```{r practice2M2}
# define prior
prior <- c(rep(0, 10), rep(1, 10))

# compute likelihood at each value in grid
likelihood <- dbinom(3, size = 3, prob = p_grid)

# compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)
plot(p_grid,
     posterior,
     type = "b",
     xlab = "probability of water",
     ylab = "posterior probability")
     mtext("20 points")

##     (2) W, W, W, L
# compute likelihood at each value in grid
likelihood <- dbinom(3, size = 4, prob = p_grid)

# compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)
plot(p_grid,
     posterior,
     type = "b",
     xlab = "probability of water",
     ylab = "posterior probability")
     mtext("20 points")

     ##     (3) L, W, W, L, W, W, W
# compute likelihood at each value in grid
likelihood <- dbinom(5, size = 7, prob = p_grid)

# compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)
plot(p_grid,
     posterior,
     type = "b",
     xlab = "probability of water",
     ylab = "posterior probability")
     mtext("20 points")


```
#### 2M3. 
Suppose there are two globes, one for Earth and one for Mars. 
The Earth globe is 70\% covered in water. The Mars globe is 100\% land.
Further suppose that one of these globes --- you don't know which --- was 
tossed in the air and produced a "land" observation. Assume that each globe 
was equally
likely to be tossed. Show that the posterior probability that the globe was the
Earth, conditional on seeing "land" (Pr(Earth | land)), is 0.23.

Pr(Earth | land) is symbolized with P_Earth_g_land 

```{r practice2M3}
P_Earth <- 0.5
P_Mars <- 0.5
P_land_g_Earth <- 0.3
P_land_g_Mars <- 1.0
P_land <- P_land_g_Earth * P_Earth + P_land_g_Mars * P_Mars
P_land_a_Earth <- P_land_g_Earth * P_Earth
P_land_a_Mars <- P_land_g_Mars * P_Mars
P_Earth_g_land <- (P_land_g_Earth * P_Earth) / P_land
P_Earth_g_land

```
#### 2M4. 
Suppose you have a dock with only three cards. 
Each card has two sides, and each side is either black or white.
One card has two black sides.
The second card has one black and one white side.
The third card has two white sides.
Now suppose all three cards are placed in a bag and shuffled.
Someone reaches into the bag and pulls out a card and places it flat on a table.
A black side is shown facing up, but you don't know the color of the side 
facing down.
Show that the probability that the other side is also black is 2/3.
Use the counting method (Section 2 of the chapter) to approach this problem.
This means counting up the ways that each card could produce the observed data 
(a black side facing up on the table).

We have three cards:
(1) B/B, (2) B/W, (3) W/W
B/B can produce this outcome 2 ways B/B -> {[B], [B]}
B/W can produce this outcome 1 way B/W -> {[B]}
W/W can produce this outcome 0 ways W/W -> {NULL}

Thus, W/W cannot have been selected and there are 3 paths that could have been
selected. Two of the three come from the B/B card so the 
Pr{[B] being on the other side of the card} is 2/3.

#### 2M5. Now suppose there are four cards: B/B, B/W, W/W, and another B/B.
Again suppose a card is drawn from the bag and a black side appears face up.
Again calculate the probability that the other side is black.

We have four cards:
(1) B/B, (2) B/W, (3) W/W, (4) B/B
B/B can produce this outcome 2 ways B/B -> {[B], [B]}
B/W can produce this outcome 1 way B/W -> {[B]}
W/W can produce this outcome 0 ways W/W -> {NULL}
B/B can produce this outcome 2 ways B/B -> {[B], [B]}

Thus, W/W cannot have been selected and there are 5 paths that could have been
selected. Four of the five come from a B/B card so the 
Pr{[B] being on the other side of the card} is 4/5.

#### 2M6. 
Imagine that black ink is heavy, and so cards with black sides are heavier than cards with white sides.
As a result, it's less likely that a card ith black sides is pulled from the 
bag. 
So again assume there are three cards: B/B, B/W, and W/W.
After experimenting a number of times, your conclude that for every way to pull 
the B/B card from the bag, there are 2 ways to pull the B/W card and 3 ways 
to pull the W/W card.
Again suppose that a card is pulled and a black side appears face up.
Show that the probability the other sides is black is now 0.5.
Use the counting method, as before.

We have three cards:
(1) B/B, (2) B/W, (3) W/W
B/B can produce this outcome 2 ways B/B -> {[B], [B]}
B/W can produce this outcome 1 way B/W -> {[B]}
W/W can produce this outcome 0 ways W/W -> {NULL}

However, because of the number of ways each type of card can be pulled out of 
the bag, these numbers are multiplied by those number of ways giving:


We have three cards:
(1) B/B, (2) B/W, (3) W/W
B/B can produce this outcome 2 * 1 ways B/B -> {[B], [B]}
B/W can produce this outcome 1 * 2 ways B/W -> {[B], [B]}
W/W can produce this outcome 0 * 3 ways W/W -> {NULL}

Thus, W/W cannot have been selected and there are 4 paths that could have been
selected. Two of the four come from a B/B card so the 
Pr{[B] being on the other side of the card} is 2/4 or 1/2.

#### 2M7. 
Assume again the original card problem, with a single card showing a 
black side face up.
Before looking at the other side, we draw another ard from the bag and lay it 
face up on the table.
The face that is shown on the new card is white.
Show that the probability that the first card, the one showing a black side, 
has black on its other side is not 0.75.
Use the counting method, if you can.
Hint: Treat this like the sequence of globe tosses, counting all the ways to 
see each observation, for each possible first card.

We have three cards:
(1) B/B, (2) B/W, (3) W/W
B/B can produce this outcome ways B/B -> {[B], [B]} + 
                                          B/W -> {[W]} + 
                                          W/W -> {[W], [W]}
B/W can produce this outcome B/W -> {[B]} + W/W -> {[W], [W]}
W/W cannot produce this outcome

Thuse there are 8 ways this could happen with 6/8 = 3/4 (0.75) being with the
B/B card being selected first, which allows the other side to be black.

####2H1.
Suppose there are two species of panda bear.
Both are equally common in the wild and live in the same places. 
They look exactly alike and eat the same food, and there is yet no genetic 
assay capable of telling them apart.
They differ however in their family sizes. 
Species A gives birth to twins 10\% of the time, otherwise birthing a single 
infant.
Species B births twins 20\% of the time, otherwise birthing singleton infants.
Assume thes numbers are known with certainty, from many years of field research.

Now suppose you are managing a captive panda breeding program. 
You have a new female panda of unknown species, and she has just given birth to 
twins.
What is the probability that her next birth will also be twins.

Let the two species of pandas A and B be refered to as P10 and P20 respectively 
based on their frequencies of twinning.
P10_t -> 1 * 0.1 / 
P20_t -> 2 * 0.2
1/3 * 0.1 + 2/3 * 0.2
Thus Pr{P? is P10} = 0.1 * 0.1 / (0.1 * 0.1 + 0.2 * 0.2)
```{r practice2H1}
P10_t <- 1 * 0.1 # Pr that parent in P10 is 1/2 that of it being P20
P20_t <- 2 * 0.2
P_t <- 1/3 * 0.1 + 2/3 * 0.2 # Probability that the next birth is twins.
P_t

```

####2H2. 
Recall all the facts from the problem above.
Now compute the probability that the panda we have is from species A, 
assuming we have observed only the first birth and that it was twins.

See above: 1/3

####2H3. 
Continuting on from the previous problem, suppose the same panda mother
has a second birth and it is not twins, but a signleton infant.
Compute the posterior probability that this panda is species A.

```{r practice2H3}
P10_s <- 1 * 0.9 # Pr that parent in P10 is 1/2 that of it being P20
P20_s <- 2 * 0.8
P_s <- (1/3) * 0.9 + 2/3 * 0.8 # Probability that the next birth is singleton.
P_P10_g_s <- (1/3) * 0.9 / P_s
P_P10_g_s

```
####2H4. 
A common boast of Bayesian statisticians is that Bayesian inference
makes it easy to use all of the data, even if the data are of different types.

So suppose not that a veterinarian comes along who has a new genetic test that
she claims can identify the species of our mother panda. 
But the test, like all tests, is imperfect.
This is the information you have about the tests:

    * The probability it correctly identifies a species A panda is 0.8.
    * The probability it correctly identifies a species B panda is 0.65.
    
The vet administers the test to your panda and tells you that the test is 
positive for species A.
First ignore your previous information from the births and compute the posterior
probability that your panda is species A.
Then redo your calculation, now using the birth data as well.

Unless I am missing something, the posterior probability that the panda is 
species A given only the result of the test is the probability of the test.
That is Pr{panda is species A} = 0.8

```{r practice2H4}
P10_g_t_and_test <- 1 * 0.8
P20_g_t_and_test <- 2 * 0.2
P10 <- P10_g_t_and_test / (P10_g_t_and_test + P20_g_t_and_test)
P10

```
```{r  3.1}
PrPV <- 0.95
PrPM <- 0.01
PrV <- 0.001
PrP <- PrPV * PrV + PrPM * (1 - PrV)
(PrVP <- PrPV * PrV / PrP)

```

```{r  3.2}
p_grid <- seq(from = 0, to = 1, length.out = 1000)
prior <- rep(1, 1000)
likelihood <- dbinom(6, size = 9, prob = p_grid)
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)

```

```{r  3.3}
samples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)

```

```{r  3.4}
plot(samples)
library(ggplot2)
ggplot(data.frame(sample_number = seq_along(samples), samples = samples), 
       aes(x = sample_number, y = samples)) +
  geom_point(color = "blue", alpha = 1/3) + labs(x = "proportion water(p)", 
                                                 y = "sample number")

```

```{r  3.5}
library(rethinking)
dens(samples)

```

```{r  3.6}
# add up posterior probability where p < 0.5
sum(posterior[ p_grid < 0.5 ] )

```

```{r  3.7}
sum(samples < 0.5) / 1e4
sum(samples < 0.5) / length(samples)

```

```{r  3.8}
sum(samples > 0.5 & samples < 0.75) / 1e4

```

```{r  3.9}
quantile(samples, 0.8)

```

```{r  3.10}
quantile(samples, c(0.1, 0.9))

```

```{r  3.11}
p_grid <- seq(from = 0, to = 1, length.out = 1000)
prior <- rep(1, 1000)
likelihood <- dbinom(3, size = 3, prob = p_grid)
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
samples <- sample(p_grid, size = 1e4, replace = TRUE, prob = posterior)

```

```{r  3.12}
PI(samples, prob = 0.5)

```

```{r  3.13}
HPDI(samples, prob = 0.5)

```

```{r  3.14}
p_grid[ which.max(posterior) ]

```

```{r  3.15}
chainmode(samples, adj = 0.01)

```

```{r  3.16}
mean(samples)
median(samples)

```

```{r  3.17}
sum(posterior * abs(0.5 - p_grid))

```

```{r  3.18}
loss <- sapply(p_grid, function(d) sum(posterior * abs(d - p_grid)) )

```

```{r  3.19}
p_grid[ which.min(loss) ]

```

```{r  3.20}
dbinom(0:2, size = 2, prob = 0.7)

```

```{r  3.21}
rbinom(1, size = 2, prob = 0.7)

```

```{r  3.22}
rbinom(10, size = 2, prob = 0.7)

```

```{r  3.23}
dummy_w <- rbinom(1e5, size = 2, prob = 0.7)
table(dummy_w)/1e5

```

```{r  3.24}
dummy_w <- rbinom(1e5, size = 9, prob = 0.7)
simplehist(dummy_w, xlab = "dummy water count")

```

```{r  3.25}
w <- rbinom(1e4, size = 9, prob = 0.6)
simplehist(w, xlab = "water count")

```

```{r  3.26}
w <- rbinom(1e4, size = 9, prob = samples)
simplehist(w, xlab = "water count")

```
## 3.5 Practice
###Easy.
These problems use the smaples from the posterior distribution for 
the globe tossing examle.
This code will give yo a specific set of samples, so that you can check your 
answers exactly.


```{r  3.27}
p_grid <- seq(from = 0, to = 1, length.out = 1000)
prior <- rep(1, 1000)
likelihood <- dbinom(6, size = 9, prob = p_grid)
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
set.seed(100)
samples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)

```
Use the values in _samples_ to answer the questins that follow.

####3E1. 
How much posterior probability lies below $p = 0.2$?
```{r practice3E1}
length(samples[samples < 0.2]) / length(samples)

```
####3E2. 
How much posterior probability lies above $p = 0.8$?
```{r practice3E2}
length(samples[samples > 0.8]) / length(samples)

```
####3E3. 
How much posterior probability lies betwee $p = 0.2$ and $p = 0.8$?
```{r practice3E3}
length(samples[samples < 0.8 | samples > 0.2]) / length(samples)

```
####3E4.
20\% of the posterior probability lies below which value of p?
```{r practice3E4}
ordered_samples <- sort(samples)
p_20_percent <- ordered_samples[ordered_samples ==
                                  ordered_samples[floor(length(samples) / 5)]][1]
p_20_percent
```
####3E5. 
20\% of the posterior probability lies above which value of p?
```{r practice3E5}
ordered_samples <- sort(samples)
p_80_percent <- ordered_samples[ordered_samples ==
                                  ordered_samples[floor(4 * length(samples) / 
                                                          5)]][1]
p_80_percent
```
####3E6. 
Which values of _p_ contain the narrowest interval equal to 66\% of the posterior probability?
```{r practice3E6}
HPDI(samples, 0.66)

```

####3E7. 
Which values of _p_ contain 66\% of the posterior probability, 
assuming equal posterior probability both below and above the interval?
```{r practice3E7}
quantile(samples, c(0.33 / 2, 1.0 - (0.33 / 2)))


```
### Medium
#### 3M1. 
Suppose the globe tossing data had turned out to be 8 water in 15 tosses. 
Construct the posterior distribution, using grid approximation. 
Use the same flat prior as before.
```{r practice3M1}
# define prior
p_grid <- seq(from = 0, to = 1, length.out = 1000)
prior <- rep(1, 1000)

# compute likelihood at each value in grid
likelihood <- dbinom(8, size = 15, prob = p_grid)

# compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)
plot(p_grid,
     posterior,
     type = "b",
     xlab = "probability of water",
     ylab = "posterior probability")
     mtext("1000 points")

```

####3M2. 
Draw 10,000 samples from the grid approximation from above. 
Then using the samples to calculate the 90\% HPDI for _p_.
```{r practice3M2}
length_out <- 10000
samples <- sample(p_grid, length_out, replace = TRUE, prob = posterior)
HPDI(samples, 0.9)

```
####3M3. 
Construct a posterior predictive check for this model and data.
This means simulate the distribution of samples, averaging over the posterior uncertainty in _p_.
What is the probability of observing 8 water in 15 tosses?
```{r practice3M3}
w <- rbinom(length_out, size = 15, prob = samples)
w_t <- table(w)

Pr_8_in_15 <- as.numeric(w_t)[names(w_t) == "8"] / sum(as.numeric(w_t))
Pr_8_in_15
```
####3M4. 
Using the posterior distribution constructed from the new (8/15) data, 
now calculate the probability of observing 6 water in 9 tosses.
```{r practice3M4}
## Numerator that follows is the number of water observations out of total
## Denominator is total number of observations.
Pr_w <- sum(as.numeric(w_t) * as.numeric(names(w_t))) / (15 * length_out)
w_6_of_9 <- dbinom(6, 9, Pr_w)
w_6_of_9

```
####3M5. 
Start over at 3M1, but now use a prior that is zero below $p = 0.5$ and
a constant above $p = 0.5$.
This corresponds to prior information that a majority of the Earth's surface is
water.
Repeat each problem above and compare the inferences.
What difference does the better prior make?
If it helps, compare inferences (using both priors) to the ture value $p = 0.7$.
```{r practice3M5a}
# define prior
p_grid <- seq(from = 0, to = 1, length.out = 1000)
prior <- c(rep(0, 500), rep(1, 500))

# compute likelihood at each value in grid
likelihood <- dbinom(8, size = 15, prob = p_grid)

# compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)
plot(p_grid,
     posterior,
     type = "b",
     xlab = "probability of water",
     ylab = "posterior probability")
     mtext("1000 points")

```
Draw 10,000 samples from the grid approximation from above. 
Then using the samples to calculate the 90\% HPDI for _p_.
```{r practice3M5b}
length_out <- 10000
samples <- sample(p_grid, length_out, replace = TRUE, prob = posterior)
HPDI(samples, 0.9)
HPDI(rbinom(length_out, 15, 0.7), 0.9)
```
Construct a posterior predictive check for this model and data.
This means simulate the distribution of samples, averaging over the posterior uncertainty in _p_.
What is the probability of observing 8 water in 15 tosses?
```{r practice3M5c}
w <- rbinom(length_out, size = 15, prob = samples)
w_t <- table(w)

Pr_8_in_15 <- as.numeric(w_t)[names(w_t) == "8"] / sum(as.numeric(w_t))
Pr_8_in_15
dbinom(8, 15, 0.7)
```
Using the posterior distribution constructed from the new (8/15) data, 
now calculate the probability of observing 6 water in 9 tosses.
```{r practice3M5d}
## Numerator that follows is the number of water observations out of total
## Denominator is total number of observations.
Pr_w <- sum(as.numeric(w_t) * as.numeric(names(w_t))) / (15 * length_out)
w_6_of_9 <- dbinom(6, 9, Pr_w)
w_6_of_9
dbinom(6, 9, 0.7)
```
### Hard.
__Introduction.__ The practice problems here all use the data below.
These data indicate the gender (male=1, female=0) of officially reported first
and second born children in 100 two-child families.

```{r  3.28}
birth1 <- c(1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
1, 0, 1, 1, 1, 0, 1, 1, 1, 1)
birth2 <- c(0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
0, 0, 0, 1, 1, 1, 0, 0, 0, 0)

```
So for example, the first family in the data reported a boy (1) and then a girl
(0).
The second family reported a girl (0) and then a boy (1).
The thrid family reported two girls.
You can load these two vectors int R's memeory by typing:
```{r  3.29}
library(rethinking)
data(homeworkch3)

```
Use these vectors as data.
So for example to compute the total number of boys born across all of these 
birth, you could use:
```{r  3.30}
sum(birth1) + sum(birth2)

```
####3H1.
Using grid approximation, compute the posterior distribution for the probability
of a birth being a boy.
Assume a uniform prior probability.
Which parameter value maximizes the posterior probability?
```{r practice3H1}
all_births <- c(birth1, birth2)
# define prior
p_grid <- seq(from = 0, to = 1, length.out = length_out)
prior <- rep(1, length_out)

# compute likelihood at each value in grid
likelihood <- dbinom(sum(all_births), size = length(all_births), prob = p_grid)

# compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

plot(p_grid,
     posterior,
     type = "b",
     xlab = "probability of boy",
     ylab = "posterior probability")
mtext("1000 points")

                     
```
